import whisper
import requests
import torch
import torchaudio
import os
import re
import sys
import io
import time

# âœ… í˜„ì¬ ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ZonOS ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, "Zonos-for-windows"))

# âœ… ZonOS ëª¨ë“ˆ import
from zonos.model import Zonos
from zonos.conditioning import make_cond_dict
from zonos.utils import DEFAULT_DEVICE as device

# âœ… TorchDynamo ì„¤ì • (Inductor ì˜¤ë¥˜ ë°©ì§€)
import torch._dynamo
torch._dynamo.config.suppress_errors = True
os.environ["TORCHDYNAMO_DISABLE"] = "1"

# âœ… Whisper STT ìˆ˜í–‰
audio_path = os.path.join(current_dir, "my_korean_audio.m4a")
model_whisper = whisper.load_model("small")
result = model_whisper.transcribe(audio_path, language="ko")
stt_text = result["text"]
print("ğŸ§ ìŒì„± ì¸ì‹ ê²°ê³¼:", stt_text)

# âœ… Ollama LLM í˜¸ì¶œ
prompt = f"""ë‹¹ì‹ ì€ ë°”ë‹·ì†ì„ íƒí—˜í•˜ë©° ë§Œë‚˜ëŠ” ì§€ì‹ ë§ì€ ì¸ê³µì§€ëŠ¥ ìºë¦­í„°ì…ë‹ˆë‹¤.
8~10ì„¸ ì–´ë¦°ì´ë“¤ì´ ê¶ê¸ˆí•´í•˜ëŠ” ë°”ë‹·ì† ìƒë¬¼, ì§€í˜•, ëª¨í—˜ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ë”°ëœ»í•˜ê²Œ ì„¤ëª…í•´ì¤˜ìš”.

ì¡°ê±´:  
- ì´ëª¨ì§€,ì´ëª¨í‹°ì½˜ ì‚¬ìš© ê¸ˆì§€.
- ë°˜ë³µ ë¬¸ì¥ ì‚¬ìš© ê¸ˆì§€.
- ì‰¬ìš´ ë‹¨ì–´ ì‚¬ìš©
- ë¬¸ì¥ ì§§ê²Œ
- ì¹œì ˆí•˜ê³  ê¸ì •ì ì¸ ë§íˆ¬
- ë•Œë¡œëŠ” ì˜ì„±ì–´ë‚˜ ê°íƒ„ì‚¬ í™œìš©

ì•„ì´ê°€ ì´ë ‡ê²Œ ë§í–ˆì–´ìš”:
\"{stt_text}\"

ì´ ì•„ì´ê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ, ì¦ê²ê²Œ ëŒ€ë‹µí•´ì¤˜ìš”!"""

response = requests.post(
    "http://localhost:11434/api/generate",
    json={"model": "gemma3:4b", "prompt": prompt, "stream": False}
)

# âœ… ì˜ˆì™¸ ë°©ì§€: ì‘ë‹µ í‚¤ ì²´í¬
response_json = response.json()
print("ğŸ“¦ ì‘ë‹µ ì „ì²´ JSON:", response_json)

if "response" not in response_json:
    raise ValueError(f"âŒ 'response' í‚¤ê°€ ì‘ë‹µì— ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ ë‚´ìš©: {response_json}")

ai_response = response_json["response"]
print("ğŸ¤– AI ì‘ë‹µ:", ai_response)

# âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def clean_for_tts(text: str) -> list:
    text = re.sub(r"[^\w\s,.?!ê°€-í£]", "", text)
    text = re.sub(r"\*\*(.*?)\*\*", r"\1", text)
    lines = text.strip().splitlines()
    seen = set()
    cleaned = []
    for line in lines:
        line = line.strip()
        if line and line not in seen:
            cleaned.append(line)
            seen.add(line)
    return cleaned

cleaned_lines = clean_for_tts(ai_response)

# âœ… ZonOS ëª¨ë¸ ë¡œë”©
model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-transformer", device="cuda")

# âœ… í™”ì ì„ë² ë”© ë¡œë”© ë˜ëŠ” ìƒì„±
embedding_path = os.path.join(current_dir, "speaker_embedding_korean4.pt")
maru_path = os.path.join(current_dir, "Zonos-for-windows/assets/maru.wav")

if not os.path.exists(embedding_path):
    wav, sr = torchaudio.load(maru_path)
    speaker = model.make_speaker_embedding(wav, sr)
    torch.save(speaker, embedding_path)
    print("ğŸ¤ í™”ì ì„ë² ë”© ì €ì¥ ì™„ë£Œ:", embedding_path)
else:
    speaker = torch.load(embedding_path, map_location=device)
    print("ğŸ“‚ ì €ì¥ëœ í™”ì ì„ë² ë”© ë¶ˆëŸ¬ì˜´:", embedding_path)

# âœ… ë¬¸ì¥ ë‹¨ìœ„ TTS ìƒì„± ë° ì €ì¥
output_dir = os.path.join(current_dir, "tts_chunks")
os.makedirs(output_dir, exist_ok=True)

for idx, line in enumerate(cleaned_lines):
    print(f"ğŸ™ï¸ [{idx+1}/{len(cleaned_lines)}] TTS ìƒì„± ì¤‘: {line}")
    start = time.time()

    cond_dict = make_cond_dict(
        text=line,
        speaker=speaker,
        language="ko",
        emotion=[0.8, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.3],
        speaking_rate=14.0,
        pitch_std=110
    )
    conditioning = model.prepare_conditioning(cond_dict)
    codes = model.generate(conditioning)
    wav_tensor = model.autoencoder.decode(codes).cpu()[0]

    output_path = os.path.join(output_dir, f"tts_{idx+1:02d}.wav")
    torchaudio.save(output_path, wav_tensor, model.autoencoder.sampling_rate)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path} (â±ï¸ {time.time() - start:.2f}s)")

print("ğŸ‰ ëª¨ë“  ë¬¸ì¥ TTS ì™„ë£Œ. ì €ì¥ëœ ê²½ë¡œ:", output_dir)
